---
# XXX FIXME fstab boot

# FIXME check whether the system we're going to connect against is booted into
#       some live rescue system. Or rather, check whether the system we'd
#       reimage is done already and we're just re-running this playbook.
#       Whichever way this might be discovered...
# For now, check that we're only running against hosts which have a magic
# passcode set

- hosts: 'reimage_nfs'
  accelerate: false
  gather_facts: no

  tasks:
    - set_fact:
        jumphost: 'rescue-{{ inventory_hostname }}'


    ##########################################################################
    ## Introduction: make sure we're on the correct system and can connect there

    - name: make sure we may re-image the system
      assert:
        that:
          - may_wipe_system == "YesWipeSystem"

# As we assume that the target system is bootet into an disposable rescue
# system, remove the ssh hostkeys for that rescue system from the
# ~/.ssh/known_hosts file and enter the currently active ones.
# sadly, most rescue systems don't expose the ssh fingerprint in any way.

# FIXME this records a hash for the IP address of the target system. this
# should better go to /etc/ssh/ssh_known_hosts
    - name: fetch remote ssh host key for rescue host
      delegate_to: localhost
      sudo: false
      # use the ansible_ssh_host var here, as inventory_hostname may not be resolvable yet
      shell: ssh-keyscan -t rsa {{ ansible_ssh_host }} | grep -v '^#' | grep -v '^no hostkey alg$'
      register: hostkeys
      changed_when: false
    - name: update current ssh host key for rescue host
      # FIXME this doesn't work correctly when hostkeys.stdout contains more than one key
      delegate_to: localhost
      sudo: false
      lineinfile:
        dest: "{{ lookup('env', 'HOME' )}}/.ssh/known_hosts"
        backup: yes
        line: '{{ ansible_ssh_host }},rescue-{{ hostkeys.stdout }}'
        regexp: '{{ ansible_ssh_host }}'


# FIXME temporarily disabled, because OVH rescue system is mounted read-only.
# FIXME-we need to handle that correctly
#
#    - apt: pkg={{ item }}
#      delegate_to: '{{ jumphost }}'
#      with_items:
#        - dosfstools
#        - cryptsetup
#        - debootstrap



    # Connect to rescue system and update the hostname.
    # Primarilly, this is to check whether we can connect at all: if this step
    # fails, you're probably running an older ansible version which does not
    # set the remote user name for the delegation, or can't connect there for
    # some other reason.

    - name: set hostname of remote system
      hostname: "name={{ inventory_hostname }}"
      delegate_to: '{{ jumphost }}'


    ###########################################################################
    ## Scene 1: configure target system's devices
    # reformat hard drives and setup raid and so on

    - name: discover device name
      command: 'readlink -f {{ boot_disk }}'
      register: readlink
      delegate_to: '{{ jumphost }}'

    - set_fact:
        boot_disk: '{{ readlink.stdout }}'

    - set_fact:
        dev_boot: '{{ boot_disk }}1'
        dev_pv: '{{ boot_disk }}2'

    - name: disable auto-configured lvm, if it's there
      shell: vgchange -an
      delegate_to: '{{ jumphost }}'

    - name: copy partition table dump
      copy: src=files/sfdisk.boot+lvm dest=/tmp/sfdisk.boot+lvm
      delegate_to: '{{ jumphost }}'

    - name: partition boot device
      shell: 'sfdisk -L {{ boot_disk }} < /tmp/sfdisk.boot+lvm'
      delegate_to: '{{ jumphost }}'

    - name: format boot partition
      shell: 'mkfs.ext3 -L boot {{ dev_boot }}'
      delegate_to: '{{ jumphost }}'

    - name: create pv
      shell: pvcreate --force --force --yes {{ dev_pv }}
      delegate_to: '{{ jumphost }}'

    - name: create vg
      shell: vgcreate vg {{ dev_pv }}
      delegate_to: '{{ jumphost }}'

    - name: create root volume
      shell: lvcreate -L 8G -n root vg
      delegate_to: '{{ jumphost }}'

    - name: format root volume
      shell: mkfs.ext4 -E stride=16384,stripe-width=16384,lazy_itable_init -L root /dev/vg/root
      delegate_to: '{{ jumphost }}'

    - name: create mountpoint for system installation
      shell: mkdir -p /mnt/root
      delegate_to: '{{ jumphost }}'

    - name: mount root file system
      shell: mount /dev/vg/root /mnt/root
      delegate_to: '{{ jumphost }}'

    - shell: mkdir /mnt/root/boot
      delegate_to: '{{ jumphost }}'

    - shell: mount {{ dev_boot }} /mnt/root/boot
      delegate_to: '{{ jumphost }}'

    ###########################################################################
    ## Scene 2: install target system os into chroot

    - name: debootstrap base system
      shell: debootstrap --include lvm2,sudo,openssh-server,linux-image-amd64,grub-pc,busybox,firmware-linux-free,dropbear,kbd,console-setup,python,aptitude {{ target_release }} /mnt/root http://{{ debian_mirror | default("ftp.de.debian.org") }}/debian
      delegate_to: '{{ jumphost }}'

    - shell: for n in dev dev/pts sys proc; do mount --bind /$n /mnt/root/$n; done
      delegate_to: '{{ jumphost }}'

    - name: switch ssh port to 516
      lineinfile: "dest=/mnt/root/etc/ssh/sshd_config line='Port 516' regexp='^Port'"
      delegate_to: '{{ jumphost }}'

    - command: chroot /mnt/root service ssh start
      delegate_to: '{{ jumphost }}'

    - name: fetch remote ssh host key
      delegate_to: localhost
      sudo: false
      shell: ssh-keyscan -t ecdsa -p 516 {{ ansible_ssh_host }} | grep -v '^#' | grep -v '^no hostkey alg$'
      register: hostkeys
      changed_when: false

    - name: update current ssh host key
      delegate_to: localhost
      sudo: false
      # FIXME this doesn't work correctly when hostkeys.stdout contains more than one key
      lineinfile:
        dest: "{{ lookup('env', 'HOME' )}}/.ssh/known_hosts"
        backup: yes
        line: '{{ ansible_ssh_host }},{{ hostkeys.stdout }}'
        regexp: '{{ ansible_ssh_host }},{{ inventory_hostname }}'

    # pre-play role/prepare

    - name: Add ansible role account
      command: 'chroot /mnt/root adduser --uid 1200 --disabled-password --gecos "ansible user" {{ ansible_role_account }}'
      args:
        creates: '/mnt/root/home/{{ ansible_role_account }}'
      delegate_to: '{{ jumphost }}'

    - name: Create SSH directory
      # copy with wrong file modes as we're copying into the rescue system. will be fixed below
      file: path=/mnt/root/home/{{ ansible_role_account }}/.ssh state=directory owner=root mode=755
      delegate_to: '{{ jumphost }}'

    - name: Copy authorized_keys for ansible remote user
      # copy with wrong file modes as we're copying into the rescue system. will be fixed below
      copy: src=files/ansible_authorized_keys dest=/mnt/root/home/{{ ansible_role_account }}/.ssh/authorized_keys owner=root group=root mode=444
      delegate_to: '{{ jumphost }}'

    - name: Install sudoers
      command: chroot /mnt/root aptitude install sudo
      args:
        creates: /mnt/root/usr/bin/sudo
      delegate_to: '{{ jumphost }}'

    - name: Copy sudoers
      template: src=templates/sudoers.j2 dest=/mnt/root/etc/sudoers.d/ansible group=root owner=root mode=440
      delegate_to: '{{ jumphost }}'

# for some reason, we've lost our apt sources list
- hosts: 'reimage_nfs'
  accelerate: false
  gather_facts: no
  sudo: yes

  tasks:
    - lineinfile:
        dest: /etc/apt/sources.list
        line: 'deb http://{{ debian_mirror | default("ftp.de.debian.org") }}/debian {{ target_release }} main'
    - command: apt-get update

# second play
#   we can connect as our service user, now set it up so that ansible can run
- hosts: 'reimage_nfs'
  accelerate: false
  gather_facts: no
  sudo: yes

  tasks:
    # needed for variable discovery
    - name: install lsb-release
      apt: name="lsb-release" state=present
      register: result
      ignore_errors: True

    - name: install python-apt (for lsb-release)
      shell: aptitude install --assume-yes python-apt
      when: result|failed

    - name: install lsb-release (try again)
      apt: name="lsb-release" state=present
      when: result|failed

    - name: Create SSH directory
      file: path=/home/{{ ansible_role_account }}/.ssh state=directory owner={{ ansible_role_account }} mode=700

    - name: Copy authorized_keys for ansible remote user
      copy: src=ansible_authorized_keys dest=/home/{{ ansible_role_account }}/.ssh/authorized_keys owner={{ ansible_role_account }} group={{ ansible_role_account }} mode=400

# third play
#   setup system so that it's rebootable
- hosts: 'reimage_nfs'
  accelerate: false
  sudo: yes

  tasks:
    - lineinfile:
        dest: /etc/fstab
        line: '/dev/vg/root /  ext4  errors=remount-ro,user_xattr  0  1'
    - lineinfile:
        dest: /etc/fstab
        line: 'LABEL=boot /boot ext3 defaults 0 2'
      # exists? probably because manual mount above
    - file: path=/etc/mtab state=link src=/proc/mounts

    - lineinfile: dest=/etc/default/grub regexp='GRUB_CMDLINE_LINUX_DEFAULT=' line='GRUB_CMDLINE_LINUX_DEFAULT="quiet ip={{ network.eth0.address }}::{{ network.eth0.gateway }}:{{ network.eth0.netmask }}:{{ inventory_hostname }}:eth0:none"'
    - command: update-grub
    - command: update-initramfs -k all -u
    - command: grub-install /dev/sda

    - name: Configure /etc/network/interfaces
      template: src=templates/interfaces.j2 dest=/etc/network/interfaces

#    - command: reboot
#      delegate_to: '{{ jumphost }}'

#    - name: waiting for server to come back
#      local_action: 'wait_for host={{ inventory_hostname }} port={{ ansible_ssh_port | default(22) }} state=started'
#      sudo: false

